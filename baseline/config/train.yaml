data:
  path: ../data
  save_tokenized_set: KoPatElectra_base # baseline
  train: ../data/train
  category_csv: ../data/category.csv
  train_csv: ../data/train/train.csv

  n_fold: 5
  test_fold: 0
  split_seed: 42
  debug: false

model:
  # pretrained_model_name_or_path : "/content/drive/MyDrive/졸업 논문/baseline/results/checkpoint-6000" # 'monologg/koelectra-small-v3-discriminator'
  pretrained_model_name_or_path : "./KIPIKorPatELECTRA/KorPatELECTRA/PT"
  num_labels: 564
  finetune: True

train:
  checkpoint_path: ''

  seed: 42
  deterministic: False
  gpu_id: "0"
  
  optim: adamw_torch

  use_step: True
  epochs: 10
  batch_size: 32
  learning_rate: 1e-3
  warmup_steps: -1
  lr_scheduler_type: 'linear' # 'constant'
  # early_stop_patience: 5

  output_dir: './results'
  save_total_limit: 3
  report_to: ['tensorboard']

  loss:
    name: FocalLoss
    params:
      gamma: 2 # gamma가 커질수록 어려운 데이터에 더 집중함.
      pos_weight: 472 # 한 데이터의 모든 클래스의 확률에 대해서 학습, 정답이 1개인 데이터는 pos_weight 564. 하지만 여러개면 pos_weight 낮아져야함.
