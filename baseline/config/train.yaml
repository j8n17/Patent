data:
  path: ../data
  train: ../data/train
  category_csv: ../data/category.csv
  train_csv: ../data/train/train.csv
  save_tokenized_dataset: True

  n_fold: 5
  test_fold: 0
  split_seed: 42
  debug: false

model:
  name: kobart
  # pretrained_model_name_or_path : "/content/drive/MyDrive/졸업 논문/baseline/results/checkpoint-6000" # 'monologg/koelectra-small-v3-discriminator'
  # pretrained_model_name_or_path : "./KIPIKorPatELECTRA/KorPatELECTRA/PT"
  pretrained_model_name_or_path : "gogamza/kobart-base-v2"
  num_labels: 564

train:
  hierarchical: 
    SSno: False
    Sno: False
    Mno: False
    Lno: False
    LLno: True
  checkpoint_path: ''

  seed: 42
  deterministic: False
  gpu_id: "0"
  
  optim: adamw_torch
  
  cls_head_only: False
  fine_tune: 
    enable: True
    n_layer: 2

  use_step: True
  epochs: 10
  batch_size: 16
  learning_rate: 5e-5
  warmup_steps: 0
  lr_scheduler_type: 'constant' # 'linear'
  # early_stop_patience: 5

  output_dir: './results'
  save_total_limit: 3
  report_to: ['tensorboard']

  loss:
    name: FocalLoss
    params:
      gamma: 2 # gamma가 커질수록 어려운 데이터에 더 집중함.
