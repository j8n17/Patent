{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23754,"status":"ok","timestamp":1714536830526,"user":{"displayName":"김준","userId":"04498591726207671929"},"user_tz":-540},"id":"1Z8YyHttNLLb","outputId":"5f63eb29-e407-455e-c8ec-f5f0329311aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714536832679,"user":{"displayName":"김준","userId":"04498591726207671929"},"user_tz":-540},"id":"hwWCla8pNRaN","outputId":"c640638a-d9b5-49ea-9bad-0fcaf54b1dd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/졸업 논문/baseline\n"]}],"source":["%cd /content/drive/MyDrive/졸업 논문/baseline"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714536833292,"user":{"displayName":"김준","userId":"04498591726207671929"},"user_tz":-540},"id":"BHJB4qxbNSp_","outputId":"5c984ff4-3c2a-45b8-d610-3fe54ecdadd6"},"outputs":[{"data":{"text/plain":["time.struct_time(tm_year=2024, tm_mon=5, tm_mday=1, tm_hour=13, tm_min=13, tm_sec=52, tm_wday=2, tm_yday=122, tm_isdst=0)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import time, os\n","os.environ['TZ'] = 'Asia/Seoul' # 서울 시간으로 설정\n","time.tzset()\n","\n","time.localtime()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":92638,"status":"ok","timestamp":1714536925928,"user":{"displayName":"김준","userId":"04498591726207671929"},"user_tz":-540},"id":"Mg06ye13NTv4","outputId":"55d6f7f1-e2b8-42a6-e041-f3c7c47d47a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting omegaconf (from -r requirements.txt (line 1))\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.2.1+cu121)\n","Collecting datasets (from -r requirements.txt (line 4))\n","  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.35.0 (from -r requirements.txt (line 5))\n","  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate (from -r requirements.txt (line 6))\n","  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0->-r requirements.txt (line 5))\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0->-r requirements.txt (line 5)) (4.66.2)\n","Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 1))\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 3))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 3))\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 4))\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets->-r requirements.txt (line 4))\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 4))\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (3.9.5)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0->-r requirements.txt (line 5))\n","  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 6)) (5.9.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0->-r requirements.txt (line 5)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0->-r requirements.txt (line 5)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0->-r requirements.txt (line 5)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0->-r requirements.txt (line 5)) (2024.2.2)\n","INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n","Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0->-r requirements.txt (line 5))\n","  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate (from -r requirements.txt (line 6))\n","  Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.29.1-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.29.0-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n","  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.27.1-py3-none-any.whl (279 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading accelerate-0.27.0-py3-none-any.whl (279 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from -r requirements.txt (line 4))\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements.txt (line 4))\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from -r requirements.txt (line 4))\n","  Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0->-r requirements.txt (line 5))\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.5)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets->-r requirements.txt (line 4))\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n","Building wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=dbaefac8f4eb790536f395813372d2427720cce459c831ff8f282b5c83068ae0\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, xxhash, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, datasets, accelerate\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.40.1\n","    Uninstalling transformers-4.40.1:\n","      Successfully uninstalled transformers-4.40.1\n","Successfully installed accelerate-0.29.3 antlr4-python3-runtime-4.9.3 datasets-2.14.7 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 tokenizers-0.14.1 transformers-4.35.0 xxhash-3.4.1\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"7b249216b951461c811ee9e18ea42c9f","pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install -r \"/content/drive/MyDrive/졸업 논문/colab/requirements.txt\""]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":641,"status":"ok","timestamp":1714536926566,"user":{"displayName":"김준","userId":"04498591726207671929"},"user_tz":-540},"id":"2xrpx6jmPmHf","outputId":"5fcd148a-a649-414e-921a-cd00eae0730b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed May  1 13:15:25 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"elapsed":570,"status":"ok","timestamp":1714536954721,"user":{"displayName":"김준","userId":"04498591726207671929"},"user_tz":-540},"id":"G2uXJypVU_wa","outputId":"770a38d1-f7ca-441e-f4c2-8f68f8fd6b24"},"outputs":[{"data":{"text/plain":["{'data': {'category_csv': './category.csv',\n","  'train_csv': './train.csv',\n","  'val_size': 0.05,\n","  'split_seed': 42,\n","  'debug': False},\n"," 'model': {'pretrained_model_name_or_path': 'monologg/koelectra-small-v3-discriminator',\n","  'num_labels': 564},\n"," 'train': {'seed': 42,\n","  'deterministic': False,\n","  'gpu_id': '0',\n","  'optim': 'adamw_torch',\n","  'epochs': 100,\n","  'batch_size': 64,\n","  'learning_rate': 5e-05,\n","  'warmup_steps': 200,\n","  'early_stop_patience': 5,\n","  'output_dir': './results',\n","  'save_total_limit': 3,\n","  'report_to': [],\n","  'loss': {'name': 'FocalLoss', 'params': {'gamma': 2, 'pos_weight': 564}}}}"]},"metadata":{},"output_type":"display_data"}],"source":["import yaml\n","\n","with open('./config/train.yaml') as f:\n","    display(yaml.full_load(f))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119191,"status":"ok","timestamp":1714537677148,"user":{"displayName":"김준","userId":"04498591726207671929"},"user_tz":-540},"id":"Q_eDrOfVW2ls","outputId":"bacfdcc1-db3c-4b29-cd82-c0366ecf92db"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","2024-05-01 13:26:03.444858: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-01 13:26:03.444893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-01 13:26:03.446226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-01 13:26:04.623907: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","2024-05-01 13:26:05,166 INFO set seed: 42\n","2024-05-01 13:26:05,167 INFO CUDA_VISIBLE_DEVICES: 0\n","hello\n","2024-05-01 13:26:05,167 INFO load dataset...\n","2024-05-01 13:26:08,681 INFO load model...\n","Downloading tokenizer_config.json: 100% 61.0/61.0 [00:00<00:00, 284kB/s]\n","Downloading config.json: 100% 458/458 [00:00<00:00, 2.33MB/s]\n","Downloading vocab.txt: 100% 263k/263k [00:00<00:00, 35.1MB/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","Downloading pytorch_model.bin: 100% 56.6M/56.6M [00:00<00:00, 304MB/s]\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","2024-05-01 13:26:13,775 INFO preprocess dataset...\n","Map: 100% 60000/60000 [00:07<00:00, 8014.77 examples/s]\n","2024-05-01 13:26:21,273 INFO tokenize dataset...\n","Map: 100% 60000/60000 [01:19<00:00, 758.43 examples/s]\n","2024-05-01 13:27:40,402 INFO split dataset...\n","2024-05-01 13:27:40,423 INFO DatasetDict({\n","    train: Dataset({\n","        features: ['documentId', 'texts', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 57000\n","    })\n","    test: Dataset({\n","        features: ['documentId', 'texts', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 3000\n","    })\n","})\n","2024-05-01 13:27:40,424 INFO build trainer...\n","/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n","  warnings.warn(\n","2024-05-01 13:27:40,724 INFO trainer: TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=./results/runs/May01_13-27-40_885fd3c778b6,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=100,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=./results,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=64,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=[],\n","resume_from_checkpoint=None,\n","run_name=./results,\n","save_on_each_node=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=200,\n","weight_decay=0.0,\n",")\n","  0% 0/89100 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","  0% 12/89100 [00:14<27:26:54,  1.11s/it]Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/졸업 논문/baseline/train.py\", line 191, in <module>\n","    main(cfg)\n","  File \"/content/drive/MyDrive/졸업 논문/baseline/train.py\", line 186, in main\n","    trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1838, in _inner_training_loop\n","    for step, inputs in enumerate(epoch_iterator):\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\", line 462, in __iter__\n","    next_batch = next(dataloader_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n","    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n","    data = self.dataset.__getitems__(possibly_batched_index)\n","  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 2807, in __getitems__\n","    batch = self.__getitem__(keys)\n","  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\n","    return self._getitem(key)\n","  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 2788, in _getitem\n","    formatted_output = format_table(\n","  File \"/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\", line 629, in format_table\n","    return formatter(pa_table, query_type=query_type)\n","  File \"/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\", line 400, in __call__\n","    return self.format_batch(pa_table)\n","  File \"/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\", line 448, in format_batch\n","    batch = self.python_arrow_extractor().extract_batch(pa_table)\n","  File \"/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\", line 150, in extract_batch\n","    return pa_table.to_pydict()\n","KeyboardInterrupt\n","  0% 12/89100 [00:14<30:08:53,  1.22s/it]\n","^C\n"]}],"source":["!python train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOLS2NfrR1Mu"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign() # 그냥 끄는게 아니라 구글 드라이브에 파일이 업데이트 되고 꺼야함. 근데 이걸 여기서 확인할 방법이 없음. 여기서는 파일 찾으면 있는데 실제로 구글드라이브에는 업로드가 안된 상태일 수 있기 때문."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXCp6scsU_wa","outputId":"55f44e26-dbe0-4e7d-a656-5fb86a4bf6f7"},"outputs":[{"data":{"text/plain":["{'data': {'category_csv': './category.csv',\n","  'test_csv': '../data/test/test_input.csv',\n","  'debug': False},\n"," 'model': {'pretrained_model_name_or_path': './results/checkpoint-22300'},\n"," 'pred': {'batch_size': 64,\n","  'threshold': 2.0,\n","  'submission_csv': './results/checkpoint-22300/submission.csv',\n","  'device': 'cuda:0'}}"]},"metadata":{},"output_type":"display_data"}],"source":["import yaml\n","\n","with open('./config/pred.yaml') as f:\n","    display(yaml.full_load(f))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397680,"status":"ok","timestamp":1714199973709,"user":{"displayName":"김준","userId":"04498591726207671929"},"user_tz":-540},"id":"InB0mzvJNXx9","outputId":"85b30b51-6b9e-43a2-8d32-deaf21556643"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","2024-04-27 15:33:05.292919: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-27 15:33:05.292960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-27 15:33:05.294445: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-27 15:33:06.455575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-27 15:33:07,193 INFO load dataset...\n","2024-04-27 15:33:10,260 INFO load model...\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","2024-04-27 15:33:31,279 INFO preprocess dataset...\n","Map: 100% 10000/10000 [00:00<00:00, 11577.22 examples/s]\n","2024-04-27 15:33:32,148 INFO tokenize dataset...\n","Map: 100% 10000/10000 [00:12<00:00, 831.25 examples/s]\n","2024-04-27 15:33:44,190 INFO model to device...\n","2024-04-27 15:33:44,654 INFO predict...\n","100% 50/50 [05:44<00:00,  6.88s/it]\n","threshold : 0.8775000000000124\n","(10000, 564)\n"]}],"source":["!python pred.py --config './config/pred.yaml'"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
